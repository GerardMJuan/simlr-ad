{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping clustering procedure\n",
    "\n",
    "We use the bootstrap technique to train the clustering 1000 times, with different samples. This way, we should be able to obtain a better picture of the resulting space. The exact procedure is, for each iteration:\n",
    "* Obtain a random subsampling of the data.\n",
    "* Compute the clustering\n",
    "* Calculate Jaccard coeficient between original clusters and new. Record highest Jaccard coeficient.\n",
    "In the end, compute median of the jaccard coeficients. This procedure is similar to clusterboot() algorithm in R, to account for stability in the clustering and find if we are actually finding relevant clusters or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import simlr_ad\n",
    "import pandas as pd\n",
    "from utils.data_utils import load_all_data, load_covariates\n",
    "from utils.utils import compute_cimlr, feat_ranking, estimate_number_clusters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the procedure\n",
    "clusters = 4\n",
    "stab_limit = 0.5 # if the stability of a said cluster is dissolved, it records.\n",
    "rd_seed = 1714                                          # Random seed for experiment replication\n",
    "\n",
    "# Paths\n",
    "existing_cluster = True                               # Compute the clustering again or use an existing one\n",
    "cluster_path = \"/home/gerard/Documents/EXPERIMENTS/SIMLR-AD/cimlr4long/\"# Path of the existing cluster, if applicable\n",
    "covariate_path = \"../long_data/covariates_long.csv\"       # Path of the covariance data frame (.csv)\n",
    "\n",
    "# Parameters of the cluster creation\n",
    "config_file = \"../configs/config_cimlr_long.ini\"               # Configuration file for the clustering computation\n",
    "output_directory_name = \"bootstrap\"\n",
    "\n",
    "# Testing parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_data, cov_names = load_covariates(covariate_path, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "if existing_cluster:\n",
    "    # Load existent\n",
    "    c_data = pd.read_csv(cluster_path + 'cluster_data.csv')\n",
    "    c_data.reset_index(inplace=True)\n",
    "else:\n",
    "    # Compute base clustering\n",
    "    y_b, S, F, ydata, alpha = compute_simlr(\n",
    "        np.array(covariate_data_new[cov_names]), clusters)\n",
    "\n",
    "print(len(c_data))\n",
    "print(len(covariate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "## Test outlier detection\n",
    "from sklearn import svm\n",
    "clf = svm.OneClassSVM(kernel=\"rbf\")\n",
    "clf.fit(covariate_data[cov_names])\n",
    "y_pred = clf.predict(covariate_data[cov_names])\n",
    "n_error_outliers = y_pred[y_pred == -1].size\n",
    "print(n_error_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation finished\n",
      "Cluster 1: 0.44719138438357703 Jaccard score.\n",
      "It got dissolved 65.0, 65.0% of the time.\n",
      "Cluster 2: 0.4287163551050427 Jaccard score.\n",
      "It got dissolved 72.0, 72.0% of the time.\n",
      "Cluster 3: 0.3164895154490641 Jaccard score.\n",
      "It got dissolved 99.0, 99.0% of the time.\n",
      "Cluster 4: 0.4868191216726101 Jaccard score.\n",
      "It got dissolved 52.0, 52.0% of the time.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# array where the number of times a cluster is dissolved (Jaccard coeficient < stab_limit)\n",
    "n_diss = np.zeros(clusters)\n",
    "niterations=100\n",
    "# array of arrays where all the coefficients obtained will be stored.\n",
    "j_coeff = np.zeros((clusters,niterations))\n",
    "# Base labels\n",
    "for i in range(niterations):\n",
    "    # Subsample\n",
    "    boot_data = covariate_data.sample(n=len(covariate_data), replace=True)\n",
    "    # Compute it\n",
    "    y_it, S, F, ydata, alpha = compute_cimlr(\n",
    "       np.array(boot_data[cov_names]), clusters)\n",
    "    # y_it = np.random.randint(1,clusters+1, size=len(boot_data))\n",
    "    # km = KMeans(n_clusters=clusters, random_state = rd_seed).fit(boot_data[cov_names])\n",
    "    # y_it = km.labels_ + 1\n",
    "    # Assign clusters\n",
    "    for c in range(1, clusters+1):\n",
    "        # For each of the original clusters\n",
    "        # And that PTID is included in PTID\n",
    "        cond = (c_data.C.values == c)\n",
    "        set_b = c_data[cond].PTID.values\n",
    "        set_b = set_b[np.in1d(set_b, boot_data.PTID.values)]\n",
    "        max_js = 0.0\n",
    "        for k in range(1, clusters+1):\n",
    "            # Create new set of clusters\n",
    "            cond = (y_it == k)\n",
    "            set_it = boot_data[cond].PTID.values\n",
    "            # set_it = set_it[np.in1d(set_it, boot_data.PTID.values)]\n",
    "            # compute jaccard score between base assignation and given cluster\n",
    "            inter = set([x for x in set_b if x in set_it])\n",
    "            union = set(list(set_b) + list(set_it))\n",
    "            js = float(len(inter) / len(union))\n",
    "            # If larger, get it\n",
    "            if js > max_js:\n",
    "                max_js = js\n",
    "        # If it dissolves, we want to record it\n",
    "        if max_js < stab_limit:\n",
    "            n_diss[c-1] += 1\n",
    "        # Save jaccard scores\n",
    "        j_coeff[c-1,i] = max_js\n",
    "    \n",
    "print('Computation finished')\n",
    "for c in range(1,clusters+1):\n",
    "    print('Cluster ' + str(c) + ': ' + str(np.mean(j_coeff[c-1,:])) + \" Jaccard score.\")\n",
    "    print(\"It got dissolved \" + str(n_diss[c-1]) + \", \" + str((n_diss[c-1]/niterations)* 100) + \"% of the time.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same procedure, but with synthetic data. This way, we can directly compare with data that is well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 146)\n",
      "(0, 146)\n",
      "(130, 146)\n",
      "(96, 146)\n",
      "(130, 146)\n",
      "(110, 146)\n",
      "(130, 146)\n",
      "(130, 146)\n",
      "(336, 146)\n"
     ]
    }
   ],
   "source": [
    "## Create synthetic data of the same size\n",
    "n_samples = []\n",
    "for c in range(0, clusters):\n",
    "    n_samples.append(len(c_data[c_data.C.values == c]))\n",
    "\n",
    "max_samples = max(n_samples)\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "# Create a blob for each cluster with the corresponding number of samples\n",
    "X, y = make_blobs(n_samples=max_samples*clusters, n_features = len(cov_names), centers = clusters, cluster_std=20.0)\n",
    "# For each cluster, select only as many elements as members of the cluster\n",
    "synth_X = []\n",
    "synth_y = []\n",
    "for c in range(0, clusters):\n",
    "    curr_items = X[y==c]\n",
    "    print(curr_items.shape)\n",
    "    curr_items = curr_items[:n_samples[c],:]\n",
    "    print(curr_items.shape)\n",
    "    synth_X.append(curr_items)\n",
    "    synth_y += ([c+1] * n_samples[c])\n",
    "\n",
    "synth_X = np.concatenate((synth_X[0],synth_X[1],synth_X[2],synth_X[3]))\n",
    "synth_data = pd.DataFrame(synth_X)\n",
    "print(synth_data.shape)\n",
    "synth_y = np.array(synth_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation finished\n",
      "Cluster 1: 0.0 Jaccard score.\n",
      "It got dissolved 5.0, 100.0% of the time.\n",
      "Cluster 2: 0.8191973424103122 Jaccard score.\n",
      "It got dissolved 0.0, 0.0% of the time.\n",
      "Cluster 3: 0.8711991889587798 Jaccard score.\n",
      "It got dissolved 0.0, 0.0% of the time.\n",
      "Cluster 4: 0.8481057921525675 Jaccard score.\n",
      "It got dissolved 0.0, 0.0% of the time.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# array where the number of times a cluster is dissolved (Jaccard coeficient < stab_limit)\n",
    "n_diss = np.zeros(clusters)\n",
    "niterations=5\n",
    "# array of arrays where all the coefficients obtained will be stored.\n",
    "j_coeff = np.zeros((clusters,niterations))\n",
    "# Base labels\n",
    "for i in range(niterations):\n",
    "    # Subsample\n",
    "    boot_data = synth_data.sample(n=len(synth_data), replace=True)\n",
    "    # Compute it\n",
    "    # y_it, S, F, ydata, alpha = compute_cimlr(\n",
    "    #   np.array(boot_data), clusters)\n",
    "    # y_it = np.random.randint(1,clusters+1, size=len(boot_data))\n",
    "    km = KMeans(n_clusters=clusters, random_state = rd_seed).fit(boot_data)\n",
    "    y_it = km.labels_ + 1\n",
    "    # Assign clusters\n",
    "    for c in range(1, clusters+1):\n",
    "        # For each of the original clusters\n",
    "        # And that PTID is included in PTID\n",
    "        cond = (synth_y == c)\n",
    "        set_b = synth_data[cond].index.values\n",
    "        set_b = set_b[np.in1d(set_b, boot_data.index.values)]\n",
    "        max_js = 0.0\n",
    "        for k in range(1, clusters+1):\n",
    "            # Create new set of clusters\n",
    "            cond = (y_it == k)\n",
    "            set_it = boot_data[cond].index.values\n",
    "            # set_it = set_it[np.in1d(set_it, boot_data.PTID.values)]\n",
    "            # compute jaccard score between base assignation and given cluster\n",
    "            inter = set([x for x in set_b if x in set_it])\n",
    "            union = set(list(set_b) + list(set_it))\n",
    "            js = float(len(inter) / len(union))\n",
    "            # If larger, get it\n",
    "            if js > max_js:\n",
    "                max_js = js\n",
    "        # If it dissolves, we want to record it\n",
    "        if max_js < stab_limit:\n",
    "            n_diss[c-1] += 1\n",
    "        # Save jaccard scores\n",
    "        j_coeff[c-1,i] = max_js\n",
    "    \n",
    "print('Computation finished')\n",
    "for c in range(1,clusters+1):\n",
    "    print('Cluster ' + str(c) + ': ' + str(np.mean(j_coeff[c-1,:])) + \" Jaccard score.\")\n",
    "    print(\"It got dissolved \" + str(n_diss[c-1]) + \", \" + str((n_diss[c-1]/niterations)* 100) + \"% of the time.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
