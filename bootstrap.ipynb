{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping clustering procedure\n",
    "\n",
    "We use the bootstrap technique to train the clustering 1000 times, with different samples. This way, we should be able to obtain a better picture of the resulting space. The exact procedure is, for each iteration:\n",
    "* Obtain a random subsampling of the data.\n",
    "* Compute the clustering\n",
    "* Calculate Jaccard coeficient between original clusters and new. Record highest Jaccard coeficient.\n",
    "In the end, compute median of the jaccard coeficients. This procedure is similar to clusterboot() algorithm in R, to account for stability in the clustering and find if we are actually finding relevant clusters or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import simlr_ad\n",
    "import pandas as pd\n",
    "from utils.data_utils import load_all_data\n",
    "from utils.utils import compute_simlr, feat_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the procedure\n",
    "niterations = 100\n",
    "clusters = 3\n",
    "stab_limit = 0.5 # if the stability of a said cluster is dissolved, it records.\n",
    "rd_seed = 1714                                          # Random seed for experiment replication\n",
    "\n",
    "# Paths\n",
    "existing_cluster = True                               # Compute the clustering again or use an existing one\n",
    "cluster_path = \"results/extendeddata_cluster/cluster_data.csv\"   # Path of the existing cluster, if applicable\n",
    "covariate_path = \"data/useddata_homo_abeta_plasma_meta.csv\"                 # Path of the covariance data frame (.csv)\n",
    "feature_path = \"data/UCSDVOL.csv\"                     # Path of the feature path (.csv)\n",
    "\n",
    "# Parameters of the cluster creation\n",
    "config_file = \"configs/config_base.ini\"               # Configuration file for the clustering computation\n",
    "output_directory_name = \"bootstrap\"\n",
    "\n",
    "# Testing parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_data, cov_names, feature_data, feature_names = load_all_data(covariate_path, feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_cluster:\n",
    "    # Load existent\n",
    "    c_data = pd.read_csv(cluster_path)\n",
    "else:\n",
    "    # Compute base clustering\n",
    "    y_b, S, F, ydata, alpha = compute_simlr(\n",
    "        np.array(covariate_data_new[cov_names]), clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "## Test outlier detection\n",
    "from sklearn import svm\n",
    "clf = svm.OneClassSVM(kernel=\"rbf\")\n",
    "clf.fit(covariate_data[cov_names])\n",
    "y_pred = clf.predict(covariate_data[cov_names])\n",
    "n_error_outliers = y_pred[y_pred == -1].size\n",
    "print(n_error_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation finished\n",
      "Cluster 1: 0.5511451669506124 Jaccard score.\n",
      "It got dissolved 42.0, 42.0% of the time.\n",
      "Cluster 2: 0.6020502050229517 Jaccard score.\n",
      "It got dissolved 31.0, 31.0% of the time.\n",
      "Cluster 3: 0.33888082055997193 Jaccard score.\n",
      "It got dissolved 94.0, 94.0% of the time.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# array where the number of times a cluster is dissolved (Jaccard coeficient < stab_limit)\n",
    "n_diss = np.zeros(clusters)\n",
    "niterations=100\n",
    "# array of arrays where all the coefficients obtained will be stored.\n",
    "j_coeff = np.zeros((clusters,niterations))\n",
    "# Base labels\n",
    "for i in range(niterations):\n",
    "    # Subsample\n",
    "    boot_data = covariate_data.sample(n=len(covariate_data), replace=True)\n",
    "    # Compute it\n",
    "    y_it, S, F, ydata, alpha = compute_simlr(\n",
    "       np.array(boot_data[cov_names]), clusters)\n",
    "    # y_b = np.random.randint(1,clusters+1, size=len(boot_data))\n",
    "    # km = KMeans(n_clusters=clusters, random_state = rd_seed).fit(covariate_data[cov_names])\n",
    "    # y_b = km.labels_ + 1\n",
    "    # Assign clusters\n",
    "    for c in range(1, clusters+1):\n",
    "        # For each of the original clusters\n",
    "        # And that PTID is included in PTID\n",
    "        cond = (c_data.C.values == c)\n",
    "        set_b = c_data[cond].PTID.values\n",
    "        set_b = set_b[np.in1d(set_b, boot_data.PTID.values)]\n",
    "        max_js = 0.0\n",
    "        for k in range(1, clusters+1):\n",
    "            # Create new set of clusters\n",
    "            cond = (y_it == k)\n",
    "            set_it = boot_data[cond].PTID.values\n",
    "            # set_it = set_it[np.in1d(set_it, boot_data.PTID.values)]\n",
    "            # compute jaccard score between base assignation and given cluster\n",
    "            inter = set([x for x in set_b if x in set_it])\n",
    "            union = set(list(set_b) + list(set_it))\n",
    "            js = float(len(inter) / len(union))\n",
    "            # If larger, get it\n",
    "            if js > max_js:\n",
    "                max_js = js\n",
    "        # If it dissolves, we want to record it\n",
    "        if max_js < stab_limit:\n",
    "            n_diss[c-1] += 1\n",
    "        # Save jaccard scores\n",
    "        j_coeff[c-1,i] = max_js\n",
    "    \n",
    "print('Computation finished')\n",
    "for c in range(1,clusters+1):\n",
    "    print('Cluster ' + str(c) + ': ' + str(np.mean(j_coeff[c-1,:])) + \" Jaccard score.\")\n",
    "    print(\"It got dissolved \" + str(n_diss[c-1]) + \", \" + str((n_diss[c-1]/niterations)* 100) + \"% of the time.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
