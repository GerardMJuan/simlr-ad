{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one out classification\n",
    "This file runs a leave one out classification in each of the clusters to check if the discriminative performance in each of the clusters is better than the classification performance in the whole.\n",
    "\n",
    "The process goes like this: given a set clustering, we perform leave one out classificaton tests in each of the cluster for the following problems and with the following classifiers:\n",
    "* In the AD/CN, AD/MCI and MCI/AD tasks.\n",
    "* Using linear regression, linear SVM, RBF SVM and random forests.\n",
    "\n",
    "We use leave one out validation because other forms of validation, such as 10-fold CV, would not work, as we do not have enough data in some of the clusters/problems to work with.\n",
    "\n",
    "It is not very useful, because sample sizes of each label are very dispar. Need to weight it someway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include and load packages, config files\n",
    "\n",
    "import numpy as np\n",
    "import simlr_ad\n",
    "import pandas as pd\n",
    "from utils.data_utils import load_all_data\n",
    "from utils.utils import compute_simlr, feat_ranking\n",
    "\n",
    "# Parameters of the procedure\n",
    "clusters = 3\n",
    "rd_seed = 1714                                          # Random seed for experiment replication\n",
    "\n",
    "# Paths\n",
    "existing_cluster = True                               # Compute the clustering again or use an existing one\n",
    "cluster_path = \"results/extendeddata_cluster/cluster_data.csv\"   # Path of the existing cluster, if applicable\n",
    "covariate_path = \"data/useddata_homo_abeta_plasma_meta.csv\"                 # Path of the covariance data frame (.csv)\n",
    "feature_path = \"data/UCSDVOL.csv\"                     # Path of the feature path (.csv)\n",
    "\n",
    "covariate_data, cov_names, feature_data, feature_names = load_all_data(covariate_path, feature_path)\n",
    "feature_data['DX'] = covariate_data.DX_bl.values\n",
    "\n",
    "if existing_cluster:\n",
    "    # Load existent\n",
    "    c_data = pd.read_csv(cluster_path)\n",
    "else:\n",
    "    # Compute base clustering\n",
    "    y_b, S, F, ydata, alpha = compute_simlr(\n",
    "        np.array(covariate_data_new[cov_names]), clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define two loops:\n",
    "* For each cluster,\n",
    "* For each possible problem in the cluster AD/MCI, AD/CN MCI/AD\n",
    "\n",
    "And, in each of the iterations, do a leave one out classification procedure with each of the classifiers:\n",
    "* linear reg\n",
    "* log reg\n",
    "* lin svm\n",
    "* rbf svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cluster: 1\n",
      "For the classification problem of AD vs LMCI\n",
      "AD samples: 20\n",
      "LMCI samples: 68\n",
      "Accuracy of linear SVM: 0.7272727272727273\n",
      "Accuracy of DecisionTreeClassifier: 0.7272727272727273\n",
      "Accuracy of Naive Bayes: 0.7613636363636364\n",
      "Accuracy of RBF SVM: 0.7272727272727273\n",
      "Results for cluster: 1\n",
      "For the classification problem of CN vs AD\n",
      "CN samples: 24\n",
      "AD samples: 20\n",
      "Accuracy of linear SVM: 0.8181818181818182\n",
      "Accuracy of DecisionTreeClassifier: 0.7727272727272727\n",
      "Accuracy of Naive Bayes: 0.8409090909090909\n",
      "Accuracy of RBF SVM: 0.8636363636363636\n",
      "Results for cluster: 1\n",
      "For the classification problem of LMCI vs CN\n",
      "LMCI samples: 68\n",
      "CN samples: 24\n",
      "Accuracy of linear SVM: 0.7065217391304348\n",
      "Accuracy of DecisionTreeClassifier: 0.5978260869565217\n",
      "Accuracy of Naive Bayes: 0.7608695652173914\n",
      "Accuracy of RBF SVM: 0.7065217391304348\n",
      "Results for cluster: 2\n",
      "For the classification problem of AD vs LMCI\n",
      "AD samples: 36\n",
      "LMCI samples: 75\n",
      "Accuracy of linear SVM: 0.5765765765765766\n",
      "Accuracy of DecisionTreeClassifier: 0.6486486486486487\n",
      "Accuracy of Naive Bayes: 0.5855855855855856\n",
      "Accuracy of RBF SVM: 0.5765765765765766\n",
      "Results for cluster: 2\n",
      "For the classification problem of CN vs AD\n",
      "CN samples: 19\n",
      "AD samples: 36\n",
      "Accuracy of linear SVM: 0.8363636363636363\n",
      "Accuracy of DecisionTreeClassifier: 0.7636363636363637\n",
      "Accuracy of Naive Bayes: 0.8181818181818182\n",
      "Accuracy of RBF SVM: 0.8\n",
      "Results for cluster: 2\n",
      "For the classification problem of LMCI vs CN\n",
      "LMCI samples: 75\n",
      "CN samples: 19\n",
      "Accuracy of linear SVM: 0.7340425531914894\n",
      "Accuracy of DecisionTreeClassifier: 0.7659574468085106\n",
      "Accuracy of Naive Bayes: 0.7021276595744681\n",
      "Accuracy of RBF SVM: 0.7127659574468085\n",
      "Results for cluster: 3\n",
      "For the classification problem of AD vs LMCI\n",
      "AD samples: 29\n",
      "LMCI samples: 18\n",
      "Accuracy of linear SVM: 0.574468085106383\n",
      "Accuracy of DecisionTreeClassifier: 0.5531914893617021\n",
      "Accuracy of Naive Bayes: 0.6595744680851063\n",
      "Accuracy of RBF SVM: 0.7446808510638298\n",
      "Results for cluster: 3\n",
      "For the classification problem of CN vs AD\n",
      "CN samples: 9\n",
      "AD samples: 29\n",
      "Accuracy of linear SVM: 0.868421052631579\n",
      "Accuracy of DecisionTreeClassifier: 0.7894736842105263\n",
      "Accuracy of Naive Bayes: 0.8421052631578947\n",
      "Accuracy of RBF SVM: 0.8157894736842105\n",
      "Results for cluster: 3\n",
      "For the classification problem of LMCI vs CN\n",
      "LMCI samples: 18\n",
      "CN samples: 9\n",
      "Accuracy of linear SVM: 0.6666666666666666\n",
      "Accuracy of DecisionTreeClassifier: 0.5925925925925926\n",
      "Accuracy of Naive Bayes: 0.7037037037037037\n",
      "Accuracy of RBF SVM: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model, svm, ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "for c in range(1,clusters+1):\n",
    "    # Select data clusters\n",
    "    data_c = feature_data[c_data.C.values == c]\n",
    "    probs = [('AD', 'LMCI'), ('CN', 'AD'), ('LMCI', 'CN')]\n",
    "    for p in probs:\n",
    "        print('Results for cluster: ' + str(c))\n",
    "        print('For the classification problem of ' + p[0] + ' vs ' + p[1])\n",
    "        # For each problem\n",
    "        x_1 = data_c[data_c.DX.values == p[0]]\n",
    "        x_2 = data_c[data_c.DX.values == p[1]]\n",
    "        x_1 = x_1[feature_names].values.tolist()\n",
    "        x_2 = x_2[feature_names].values.tolist()\n",
    "        X = x_1 + x_2\n",
    "        print(p[0] + ' samples: ' + str(len(x_1)))\n",
    "        print(p[1] + ' samples: ' + str(len(x_2)))\n",
    "        Y = np.concatenate((np.zeros(len(x_1), dtype=np.float64), np.ones(len(x_2), dtype=np.float64)))\n",
    "        loo = LeaveOneOut()\n",
    "        splits = loo.get_n_splits(X)\n",
    "        \n",
    "        # SVM        \n",
    "        clf = svm.LinearSVC(class_weight='balanced')\n",
    "        sc1 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "        print('Accuracy of linear SVM: ' + str(np.average(sc1)))\n",
    "        \n",
    "        # Decisio nTree\n",
    "        clf = DecisionTreeClassifier(max_depth=5, class_weight='balanced')\n",
    "        sc2 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "        print('Accuracy of DecisionTreeClassifier: ' + str(np.average(sc2)))\n",
    "\n",
    "        # Naive Bayes\n",
    "        clf = GaussianNB()\n",
    "        sc3 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "        print('Accuracy of Naive Bayes: ' + str(np.average(sc3)))\n",
    "\n",
    "        # RBF SVM\n",
    "        clf = svm.SVC(class_weight='balanced')\n",
    "        sc4 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "        print('Accuracy of RBF SVM: ' + str(np.average(sc4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all the clusters: 3\n",
      "For the classification problem of AD vs LMCI\n",
      "AD samples: 85\n",
      "LMCI samples: 161\n",
      "Accuracy of linear SVM: 0.6341463414634146\n",
      "Accuracy of DecisionTreeClassifier: 0.556910569105691\n",
      "Accuracy of Naive Bayes: 0.6585365853658537\n",
      "Accuracy of RBF SVM: 0.6382113821138211\n",
      "For the classification problem of CN vs AD\n",
      "CN samples: 52\n",
      "AD samples: 85\n",
      "Accuracy of linear SVM: 0.8467153284671532\n",
      "Accuracy of DecisionTreeClassifier: 0.8321167883211679\n",
      "Accuracy of Naive Bayes: 0.781021897810219\n",
      "Accuracy of RBF SVM: 0.7883211678832117\n",
      "For the classification problem of LMCI vs CN\n",
      "LMCI samples: 161\n",
      "CN samples: 52\n",
      "Accuracy of linear SVM: 0.6854460093896714\n",
      "Accuracy of DecisionTreeClassifier: 0.6713615023474179\n",
      "Accuracy of Naive Bayes: 0.704225352112676\n",
      "Accuracy of RBF SVM: 0.6995305164319249\n"
     ]
    }
   ],
   "source": [
    "## For all the clusters\n",
    "probs = [('AD', 'LMCI'), ('CN', 'AD'), ('LMCI', 'CN')]\n",
    "data_c = feature_data\n",
    "print('Results for all the clusters: ' + str(c))\n",
    "for p in probs:\n",
    "    print('For the classification problem of ' + p[0] + ' vs ' + p[1])\n",
    "    # For each problem\n",
    "    x_1 = data_c[data_c.DX.values == p[0]]\n",
    "    x_2 = data_c[data_c.DX.values == p[1]]\n",
    "    x_1 = x_1[feature_names].values.tolist()\n",
    "    x_2 = x_2[feature_names].values.tolist()\n",
    "    X = x_1 + x_2\n",
    "    print(p[0] + ' samples: ' + str(len(x_1)))\n",
    "    print(p[1] + ' samples: ' + str(len(x_2)))\n",
    "    Y = np.concatenate((np.zeros(len(x_1), dtype=np.float64), np.ones(len(x_2), dtype=np.float64)))\n",
    "    loo = LeaveOneOut()\n",
    "    splits = loo.get_n_splits(X)\n",
    "\n",
    "    # SVM        \n",
    "    clf = svm.LinearSVC(class_weight='balanced')\n",
    "    sc1 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "    print('Accuracy of linear SVM: ' + str(np.average(sc1)))\n",
    "\n",
    "    # Decisio nTree\n",
    "    clf = DecisionTreeClassifier(max_depth=5, class_weight='balanced')\n",
    "    sc2 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "    print('Accuracy of DecisionTreeClassifier: ' + str(np.average(sc2)))\n",
    "\n",
    "    # Naive Bayes\n",
    "    clf = GaussianNB()\n",
    "    sc3 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "    print('Accuracy of Naive Bayes: ' + str(np.average(sc3)))\n",
    "\n",
    "    # RBF SVM\n",
    "    clf = svm.SVC(class_weight='balanced')\n",
    "    sc4 = cross_val_score(clf, X, Y, cv=loo, scoring='accuracy')\n",
    "    print('Accuracy of RBF SVM: ' + str(np.average(sc4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
