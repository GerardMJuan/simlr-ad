{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation testing\n",
    "\n",
    "This script performs permutation testing experiments: in order to assess the true differences between clusters, we need to test for differences in random clusters and compare them to the differences obtained in the actual clustering:\n",
    "\n",
    "1. We permute the data in random clusters. Do 1000 iterations.\n",
    "2. We test for differences in each iteration, for each of features.\n",
    "3. We compare the obtained differences in the real clusters to the distribution of differences over the real ones.\n",
    "\n",
    "In order to exactly reproduce the figures with the exact same values, the cells of the notebook should be reproduced in the same order. Seed can be changed to test that results are robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.data_utils import load_all_data\n",
    "from utils.utils import compute_simlr, feat_ranking\n",
    "from scipy.stats import ttest_1samp, wilcoxon, ttest_ind, mannwhitneyu\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization and loading of variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the procedure\n",
    "clusters = 4\n",
    "rd_seed = 1714                                          # Random seed for experiment replication\n",
    "test_name = 'wilc'\n",
    "feature = 'volume'                                      # either blood or volume\n",
    "\n",
    "# Paths\n",
    "existing_cluster = True                                              # Compute the clustering again or use an existing one\n",
    "cluster_path = \"/home/gerard/Documents/EXPERIMENTS/SIMLR-AD/cimlr4/\"# Path of the existing cluster, if applicable\n",
    "covariate_path = \"../data/useddata_homo_abeta_plasma_meta.csv\"       # Path of the covariance data frame (.csv)\n",
    "feature_path = \"../data/aseg_volume.csv\"                # Path of the feature file, generated with freesurfer (.csv)\n",
    "\n",
    "\n",
    "covariate_data, cov_names, feature_data, feature_names = load_all_data(covariate_path, feature_path)\n",
    "feature_data['DX'] = covariate_data.DX_bl.values\n",
    "\n",
    "if existing_cluster:\n",
    "    # Load existent\n",
    "    c_data = pd.read_csv(cluster_path + 'cluster_data.csv')\n",
    "    c_data.reset_index(inplace=True)\n",
    "\n",
    "    ## Load S, F data\n",
    "    S = np.load(cluster_path + 'S_matrix.npy')\n",
    "    ydata = np.load(cluster_path + 'ydata_matrix.npy')\n",
    "    F = np.load(cluster_path + 'F_matrix.npy')\n",
    "else:\n",
    "    # Compute base clustering\n",
    "    y_b, S, F, ydata, alpha = compute_simlr(\n",
    "        np.array(covariate_data[cov_names]), clusters)\n",
    "\n",
    "    \n",
    "# Hand made feature names\n",
    "feature_names_fig = [\"Left Lateral Ventricle\", \"Left Inf. Lateral Ventricle\", \"Left Cerebellum WM\", \"Left Cerebellum Cortex\", \"Left Thalamus Proper\", \"Left Caudate\", \n",
    "                 \"Left Putamen\", \"Left Pallidum\", \"3rd Ventricle\", \"4th Ventricle\", \"Brain Stem\", \"Left Hippocampus\", \"Left Amygdala\", \"Cerebrospinal Fluid\",\n",
    "                 \"Left Accumbens area\", \"Left Ventral Diencephalon\", \"Left Vessel\", \"Left Choroid Plexus\", \"Right Lateral Ventricle\", \"Right Inf. Lateral Ventricle\",\n",
    "                 \"Right Cerebellum WM\", \"Right Cerebellum Cortex\", \"Right Thalamus Proper\", \"Right Caudate\", \"Right Putamen\", \"Right Pallidum\", \"Right Hippocampus\", \n",
    "                 \"Right Amygdala\", \"Right Accumbens Area\", \"Right Ventral Diencephalon\", \"Right Vessel\", \"Right-Choroid-Plexus\", \"WM hypointensities\", \"Optic Chiasm\",\n",
    "                 \"CC Posterior\", \"CC Mid Posterior\", \"CC Central\", \"CC Mid Anterior\", \"CC Anterior\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the tests, according to the procedure. We have two big groups to compare:\n",
    "* Each cluster against others (whole population).\n",
    "* Within clusters, compare against each other groups (12 problems)\n",
    "* Different DX groups on whole population (3 problems)\n",
    "* For each DX group, one cluster against the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST FIGURE: Each cluster against others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations of the permutation tests\n",
    "it = 1000\n",
    "\n",
    "problems = [1,2,3,4]\n",
    "\n",
    "features = feature_names\n",
    "# problems = [(4, 'nA', 'CN'), (4, 'nA', 'LMCI'), (4, 'nA', 'AD')]\n",
    "# problems = [(1, 4, 'CN'), (1, 4, 'LMCI'), (1, 4, 'AD')]\n",
    "# problems = [(1, 3, 'CN'), (1, 3, 'LMCI'), (1, 3, 'AD')]\n",
    "# problems = [(2, 3, 'CN'), (2, 3, 'LMCI'), (2, 3, 'AD')]\n",
    "# problems = [(2, 4, 'CN'), (2, 4, 'LMCI'), (2, 4, 'AD')]\n",
    "# problems = [(3, 4, 'CN'), (3, 4, 'LMCI'), (3, 4, 'AD')]\n",
    "data = feature_data\n",
    "\n",
    "results =np.zeros((it, len(problems), len(features)))\n",
    "\n",
    "# random seeds for reproducibility\n",
    "np.random.seed(rd_seed)\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "for i in range(it):\n",
    "    # Create a random permutation\n",
    "    ## Try shuffling between clusters AND the DX\n",
    "    dx_list = ['CN', 'LMCI', 'AD']\n",
    "    \n",
    "    data_shuffled = data.copy()\n",
    "    \n",
    "    for dx in dx_list:\n",
    "        indexs = c_data.DX.values == dx\n",
    "        to_shuffle = data_shuffled[indexs]\n",
    "        to_shuffle = shuffle(to_shuffle)\n",
    "        data_shuffled.loc[indexs] = to_shuffle.values\n",
    "    \n",
    "    j = 0\n",
    "    for p in problems:\n",
    "        c1 = data_shuffled[c_data['C'].values == p]\n",
    "        c2 = data_shuffled[c_data['C'].values != p]\n",
    "        k = 0\n",
    "        for f in features:\n",
    "            # Do test\n",
    "            u, p_value = mannwhitneyu(c1[f].values, c2[f].values)            \n",
    "            # Add the test to the structure\n",
    "            results[i,j,k] = p_value\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "\n",
    "## Compute over the normal clusters\n",
    "results_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "j = 0\n",
    "for p in problems:\n",
    "    # Select the features\n",
    "    c1 = data[c_data['C'].values == p]\n",
    "    c2 = data[c_data['C'].values != p]\n",
    "    k = 0\n",
    "    for f in features:\n",
    "        # Do test\n",
    "        u, p_value = mannwhitneyu(c1[f].values, c2[f].values)\n",
    "    \n",
    "        # Add the test to the structure\n",
    "        results_base[j,k] = p_value\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we have the results, we need to:\n",
    "# For each feature and problem, do the tests over the original data and compare them to the distribution.\n",
    "# Create a figure with the distribution for each problem and feature with a bright point the true obtained difference.\n",
    "\n",
    "# Percentile files\n",
    "percentiles = np.zeros((len(problems), len(features)))\n",
    "percentiles_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# Structures containing the statistically significant files:\n",
    "p005 = []\n",
    "p001 = []\n",
    "p0001 = []\n",
    "\n",
    "for p in range(len(problems)):\n",
    "    for f in range(len(features)):\n",
    "        iterations = results[:,p,f]\n",
    "        score = percentileofscore(iterations, results_base[p,f],kind='weak')\n",
    "        if score == 0:\n",
    "            score = 0.1\n",
    "        percentiles_base[p,f] = score / 100\n",
    "        percentiles[p,f] = -np.log10(score / 100)\n",
    "        if np.isnan(percentiles[p,f]):\n",
    "            percentiles[p,f] = 0.001\n",
    "        \n",
    "        # Save is significant        \n",
    "        if percentiles_base[p,f] <= 0.001:\n",
    "            p0001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.01:\n",
    "            p001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.05:\n",
    "            p005.append((p,f))\n",
    "\n",
    "# Create dataframe\n",
    "df_percentiles = pd.DataFrame(data=percentiles, index=problems, columns=features)\n",
    "df_percentiles.to_csv('percentiles.csv')\n",
    "\n",
    "# Craete heatmap of the percentiles of each feature vs all problems\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "prob_names = [\"C1 vs Rest\", \"C2 vs Rest\", \"C3 vs Rest\", \"C4 vs Rest\"]\n",
    "\n",
    "cmap = sns.cubehelix_palette(11, reverse=False, as_cmap=True)\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "f, ax = plt.subplots(figsize=(4.5, 10))\n",
    "ax2 = sns.heatmap(df_percentiles.T, annot=percentiles_base.T, linewidths=1, ax=ax, cbar=False, cmap=cmap,\n",
    "                  cbar_kws={\"ticks\":[0,0.5,1,1.3,1.5,2,2.5,3]}, xticklabels=prob_names, yticklabels=feature_names_fig, vmin = 0, vmax = 3)\n",
    "plt.yticks(rotation=30) \n",
    "plt.xticks(rotation=30, ha=\"right\") \n",
    "\n",
    "# Draw rectangles about the significant values. The significant values should be added AUTOMATICALLY\n",
    "\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "\n",
    "\n",
    "colors = sns.color_palette(\"autumn_r\", 3)\n",
    "## Add rectangles \n",
    "for sq in p005:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[0], lw=2.5))\n",
    "\n",
    "for sq in p001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[1], lw=2.5))\n",
    "\n",
    "for sq in p0001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[2], lw=2.5))\n",
    "\n",
    "p1 = Patch(edgecolor=colors[0], fill=False, label='p < 0.05', linewidth=2)\n",
    "p2 = Patch(edgecolor=colors[1], fill=False, label='p < 0.01', linewidth=2)\n",
    "p3 = Patch(edgecolor=colors[2], fill=False, label='p < 0.001', linewidth=2)\n",
    "\n",
    "plt.legend([p1, p2, p3], ['p < 0.05','p < 0.01','p < 0.001'], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.1, frameon=False)\n",
    "## Also create a legend for the rectangles. Would be great to have three factors: <0.05, <0.01, <0.001\n",
    "# plt.savefig('clusteragainstothers.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('clusteragainstothers.eps', format='eps', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECOND FIGURE: Within clusters, against other groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations of the permutation tests\n",
    "it = 1000\n",
    "\n",
    "problems = [1,2,3,4]\n",
    "\n",
    "features = feature_names\n",
    "problems = [(1,'CN','AD'), (2,'CN','AD'), (3,'CN','AD'),(4,'CN','AD'),\n",
    "            (1,'CN','LMCI'), (2,'CN','LMCI'),(3,'CN','LMCI'), (4,'CN','LMCI'), \n",
    "            (1,'LMCI','AD'), (2,'LMCI','AD'), (3,'LMCI','AD'), (4,'LMCI','AD')]\n",
    "\n",
    "data = feature_data\n",
    "\n",
    "results = np.zeros((it, len(problems), len(features)))\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "for i in range(it):\n",
    "    # Create a random permutation\n",
    "    ## Try shuffling between clusters AND the DX\n",
    "    dx_list = ['CN', 'LMCI', 'AD']\n",
    "    \n",
    "    # data_shuffled = shuffle(data)\n",
    "    \n",
    "    data_shuffled = data.copy()\n",
    "    \n",
    "    for dx in dx_list:\n",
    "        indexs = c_data.DX.values == dx\n",
    "        to_shuffle = data_shuffled[indexs]\n",
    "        to_shuffle = shuffle(to_shuffle)\n",
    "        data_shuffled.loc[indexs] = to_shuffle.values\n",
    "    \n",
    "    j = 0\n",
    "    for p in problems:\n",
    "        # Select the features\n",
    "        c1 = data_shuffled[c_data['C'].values == p[0]]\n",
    "        c2 = data_shuffled[c_data['C'].values == p[0]]\n",
    "        c1 = c1[c1.DX.values == p[1]]\n",
    "        c2 = c2[c2.DX.values == p[2]]\n",
    "        k = 0\n",
    "        for f in features:\n",
    "            # Do test\n",
    "            u, p_value = mannwhitneyu(c1[f].values, c2[f].values)            \n",
    "            # Add the test to the structure\n",
    "            results[i,j,k] = p_value\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "\n",
    "## Compute over the normal clusters\n",
    "results_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "j = 0\n",
    "for p in problems:\n",
    "    # Select the features\n",
    "    c1 = data[c_data['C'].values == p[0]]\n",
    "    c2 = data[c_data['C'].values == p[0]]\n",
    "    c1 = c1[c1.DX.values == p[1]]\n",
    "    c2 = c2[c2.DX.values == p[2]]\n",
    "    k = 0\n",
    "    for f in features:\n",
    "        # Do test\n",
    "        u, p_value = mannwhitneyu(c1[f].values, c2[f].values)\n",
    "    \n",
    "        # Add the test to the structure\n",
    "        results_base[j,k] = p_value\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Once we have the results, we need to:\n",
    "#* For each feature and problem, do the tests over the original data and compare them to the distribution.\n",
    "#* Create a figure with the distribution for each problem and feature with a bright point the true obtained difference.\n",
    "\n",
    "# Percentile files\n",
    "percentiles = np.zeros((len(problems), len(features)))\n",
    "percentiles_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# Structures containing the statistically significant files:\n",
    "p005 = []\n",
    "p001 = []\n",
    "p0001 = []\n",
    "\n",
    "for p in range(len(problems)):\n",
    "    for f in range(len(features)):\n",
    "        iterations = results[:,p,f]\n",
    "        score = percentileofscore(iterations, results_base[p,f],kind='weak')\n",
    "        if score == 0:\n",
    "            score = 0.1\n",
    "        percentiles_base[p,f] = score / 100\n",
    "        percentiles[p,f] = -np.log10(score / 100)\n",
    "        if np.isnan(percentiles[p,f]):\n",
    "            percentiles[p,f] = 0.001\n",
    "        \n",
    "        # Save is significant        \n",
    "        if percentiles_base[p,f] <= 0.001:\n",
    "            p0001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.01:\n",
    "            p001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.05:\n",
    "            p005.append((p,f))\n",
    "\n",
    "# Create dataframe\n",
    "df_percentiles = pd.DataFrame(data=percentiles, index=problems, columns=features)\n",
    "df_percentiles.to_csv('percentiles.csv')\n",
    "\n",
    "# Craete heatmap of the percentiles of each feature vs all problems\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\")\n",
    "sns.set(font_scale=0.7)\n",
    "prob_names = [\"C1 - CN vs AD\", \"C2 - CN vs AD\", \"C3 - CN vs AD\", \"C4 - CN vs AD\",\n",
    "              \"C1 - CN vs MCI\", \"C2 - CN vs MCI\", \"C3 - CN vs MCI\", \"C4 - CN vs MCI\",\n",
    "              \"C1 - MCI vs AD\", \"C2 - MCI vs AD\", \"C3 - MCI vs AD\", \"C4 - MCI vs AD\"]\n",
    "\n",
    "cmap = sns.cubehelix_palette(11, reverse=False, as_cmap=True)\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "f, ax = plt.subplots(figsize=(5.5, 10))\n",
    "ax2 = sns.heatmap(df_percentiles.T, annot=percentiles_base.T, linewidths=1, ax=ax, cbar=False, cmap=cmap,\n",
    "                  cbar_kws={\"ticks\":[0,0.5,1,1.3,1.5,2,2.5,3]}, xticklabels=prob_names, yticklabels=feature_names_fig, vmin = 0, vmax = 3)\n",
    "plt.yticks(rotation=30) \n",
    "plt.xticks(rotation=30, ha=\"right\") \n",
    "\n",
    "# Draw rectangles about the significant values. The significant values should be added AUTOMATICALLY\n",
    "\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "colors = sns.color_palette(\"autumn_r\", 3)\n",
    "## Add rectangles \n",
    "for sq in p005:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[0], lw=2.5))\n",
    "\n",
    "for sq in p001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[1], lw=2.5))\n",
    "\n",
    "for sq in p0001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[2], lw=2.5))\n",
    "\n",
    "p1 = Patch(edgecolor=colors[0], fill=False, label='p < 0.05', linewidth=2)\n",
    "p2 = Patch(edgecolor=colors[1], fill=False, label='p < 0.01', linewidth=2)\n",
    "p3 = Patch(edgecolor=colors[2], fill=False, label='p < 0.001', linewidth=2)\n",
    "\n",
    "plt.legend([p1, p2, p3], ['p < 0.05','p < 0.01','p < 0.001'], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.1, frameon=False)\n",
    "## Also create a legend for the rectangles. Would be great to have three factors: <0.05, <0.01, <0.001\n",
    "# plt.savefig('intraclustergroups.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('intraclustergroups.eps', format='eps', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIRD FIGURE: Different DX groups over the whole population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations of the permutation tests\n",
    "\n",
    "features = feature_names\n",
    "problems = [('CN','AD'), ('CN', 'LMCI'), ('LMCI', 'AD')]\n",
    "\n",
    "data = feature_data\n",
    "\n",
    "## Compute over the normal clusters\n",
    "results_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "j = 0\n",
    "for p in problems:\n",
    "    # Select the features\n",
    "    c1 = data[data.DX.values == p[0]]\n",
    "    c2 = data[data.DX.values == p[1]]\n",
    "    k = 0\n",
    "    for f in features:\n",
    "        # Do test\n",
    "        u, p_value = mannwhitneyu(c1[f].values, c2[f].values)\n",
    "        # Add the test to the structure\n",
    "        results_base[j,k] = p_value\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Once we have the results, we need to:\n",
    "#* For each feature and problem, do the tests over the original data and compare them to the distribution.\n",
    "#* Create a figure with the distribution for each problem and feature with a bright point the true obtained difference.\n",
    "\n",
    "# Percentile files\n",
    "percentiles = np.zeros((len(problems), len(features)))\n",
    "percentiles_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# Structures containing the statistically significant files:\n",
    "p005 = []\n",
    "p001 = []\n",
    "p0001 = []\n",
    "\n",
    "for p in range(len(problems)):\n",
    "    for f in range(len(features)):\n",
    "        score = results_base[p,f]\n",
    "        if score == 0:\n",
    "            score = 0.1\n",
    "        percentiles[p,f] = -np.log10(score / 100)\n",
    "        percentiles_base[p,f] = score\n",
    "        # Save is significant        \n",
    "        if percentiles_base[p,f] <= 0.001:\n",
    "            p0001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.01:\n",
    "            p001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.05:\n",
    "            p005.append((p,f))\n",
    "\n",
    "# Create dataframe\n",
    "df_percentiles = pd.DataFrame(data=percentiles, index=problems, columns=features)\n",
    "df_percentiles.to_csv('percentiles.csv')\n",
    "\n",
    "# Craete heatmap of the percentiles of each feature vs all problems\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "sns.set(font_scale=0.9)\n",
    "f, ax = plt.subplots(figsize=(5, 10))\n",
    "\n",
    "prob_names = [\"CN vs AD\", \"CN vs MCI\", \"MCI vs AD\"]\n",
    "\n",
    "cmap = sns.cubehelix_palette(11, reverse=False, as_cmap=True)\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "ax2 = sns.heatmap(df_percentiles.T, annot=percentiles_base.T, linewidths=1, ax=ax, cbar=False, cmap=cmap,\n",
    "                  cbar_kws={\"ticks\":[0,0.5,1,1.3,1.5,2,2.5,3]}, xticklabels=prob_names, yticklabels=feature_names_fig, vmin = 0, vmax = 6)\n",
    "plt.yticks(rotation=30) \n",
    "plt.xticks(rotation=30) \n",
    "\n",
    "# Draw rectangles about the significant values. The significant values should be added AUTOMATICALLY\n",
    "\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "\n",
    "\n",
    "colors = sns.color_palette(\"autumn_r\", 3)\n",
    "## Add rectangles \n",
    "for sq in p005:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[0], lw=2.5))\n",
    "\n",
    "for sq in p001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[1], lw=2.5))\n",
    "\n",
    "for sq in p0001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[2], lw=2.5))\n",
    "\n",
    "p1 = Patch(edgecolor=colors[0], fill=False, label='p < 0.05', linewidth=2)\n",
    "p2 = Patch(edgecolor=colors[1], fill=False, label='p < 0.01', linewidth=2)\n",
    "p3 = Patch(edgecolor=colors[2], fill=False, label='p < 0.001', linewidth=2)\n",
    "\n",
    "plt.legend([p1, p2, p3], ['p < 0.05','p < 0.01','p < 0.001'], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.1, frameon=False)\n",
    "## Also create a legend for the rectangles. Would be great to have three factors: <0.05, <0.01, <0.001\n",
    "# plt.savefig('betweengroups_wholepop.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('betweengroups_wholepop.eps', format='eps', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOURTH FIGURE: For each DX group, one cluster against the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations of the permutation tests\n",
    "it = 1000\n",
    "\n",
    "features = feature_names\n",
    "problems = [(1,'CN'), (1,'LMCI'), (1,'AD'),\n",
    "            (2,'CN'), (2,'LMCI'), (2,'AD'),\n",
    "            (3,'CN'), (3,'LMCI'), (3,'AD'),\n",
    "            (4,'CN'), (4,'LMCI'), (4,'AD')]\n",
    "\n",
    "data = feature_data\n",
    "\n",
    "results =np.zeros((it, len(problems), len(features)))\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "for i in range(it):\n",
    "    # Create a random permutation\n",
    "    ## Try shuffling between clusters AND the DX\n",
    "    dx_list = ['CN', 'LMCI', 'AD']\n",
    "    \n",
    "    # data_shuffled = shuffle(data)\n",
    "    \n",
    "    data_shuffled = data.copy()\n",
    "    \n",
    "    for dx in dx_list:\n",
    "        indexs = c_data.DX.values == dx\n",
    "        to_shuffle = data_shuffled[indexs]\n",
    "        to_shuffle = shuffle(to_shuffle)\n",
    "        data_shuffled.loc[indexs] = to_shuffle.values\n",
    "    \n",
    "    j = 0\n",
    "    for p in problems:\n",
    "        # Select the features\n",
    "        c1 = data_shuffled[c_data['C'].values == p[0]]\n",
    "        c2 = data_shuffled[c_data['C'].values != p[0]]\n",
    "        c1 = c1[c1.DX.values == p[1]]\n",
    "        c2 = c2[c2.DX.values == p[1]]\n",
    "        k = 0\n",
    "        for f in features:\n",
    "            # Do test\n",
    "            u, p_value = mannwhitneyu(c1[f].values, c2[f].values)            \n",
    "            # Add the test to the structure\n",
    "            results[i,j,k] = p_value\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "\n",
    "## Compute over the normal clusters\n",
    "results_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# The randomized clusters need to be the same size of the original clusters\n",
    "j = 0\n",
    "for p in problems:\n",
    "    # Select the features\n",
    "    c1 = data[c_data['C'].values == p[0]]\n",
    "    c2 = data[c_data['C'].values != p[0]]\n",
    "    c1 = c1[c1.DX.values == p[1]]\n",
    "    c2 = c2[c2.DX.values == p[1]]\n",
    "    k = 0\n",
    "    for f in features:\n",
    "        # Do test\n",
    "        u, p_value = mannwhitneyu(c1[f].values, c2[f].values)\n",
    "    \n",
    "        # Add the test to the structure\n",
    "        results_base[j,k] = p_value\n",
    "        k += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Once we have the results, we need to:\n",
    "#* For each feature and problem, do the tests over the original data and compare them to the distribution.\n",
    "#* Create a figure with the distribution for each problem and feature with a bright point the true obtained difference.\n",
    "\n",
    "# Percentile files\n",
    "percentiles = np.zeros((len(problems), len(features)))\n",
    "percentiles_base = np.zeros((len(problems), len(features)))\n",
    "\n",
    "# Structures containing the statistically significant files:\n",
    "p005 = []\n",
    "p001 = []\n",
    "p0001 = []\n",
    "\n",
    "for p in range(len(problems)):\n",
    "    for f in range(len(features)):\n",
    "        iterations = results[:,p,f]\n",
    "        score = percentileofscore(iterations, results_base[p,f],kind='weak')\n",
    "        if score == 0:\n",
    "            score = 0.1\n",
    "        percentiles_base[p,f] = score / 100\n",
    "        percentiles[p,f] = -np.log10(score / 100)\n",
    "        if np.isnan(percentiles[p,f]):\n",
    "            percentiles[p,f] = 0.001\n",
    "        \n",
    "        # Save is significant        \n",
    "        if percentiles_base[p,f] <= 0.001:\n",
    "            p0001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.01:\n",
    "            p001.append((p,f))\n",
    "        elif percentiles_base[p,f] <= 0.05:\n",
    "            p005.append((p,f))\n",
    "\n",
    "# Create dataframe\n",
    "df_percentiles = pd.DataFrame(data=percentiles, index=problems, columns=features)\n",
    "df_percentiles.to_csv('percentiles.csv')\n",
    "\n",
    "# Craete heatmap of the percentiles of each feature vs all problems\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\")\n",
    "sns.set(font_scale=0.7)\n",
    "f, ax = plt.subplots(figsize=(5.5, 10))\n",
    "\n",
    "prob_names = [\"CN - C1 vs Rest\", \"MCI - C1 vs Rest\", \"AD - C1 vs Rest\",\n",
    "             \"CN - C2 vs Rest\", \"MCI - C2 vs Rest\", \"AD - C2 vs Rest\",\n",
    "             \"CN - C3 vs Rest\", \"MCI - C3 vs Rest\", \"AD - C3 vs Rest\",\n",
    "             \"CN - C4 vs Rest\", \"MCI - C4 vs Rest\", \"AD - C4 vs Rest\"]\n",
    "\n",
    "cmap = sns.cubehelix_palette(11, reverse=False, as_cmap=True)\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "ax2 = sns.heatmap(df_percentiles.T, annot=percentiles_base.T, linewidths=1, ax=ax, cbar=False, cmap=cmap,\n",
    "                  cbar_kws={\"ticks\":[0,0.5,1,1.3,1.5,2,2.5,3]}, xticklabels=prob_names, yticklabels=feature_names_fig, vmin = 0, vmax = 3)\n",
    "plt.yticks(rotation=30) \n",
    "plt.xticks(rotation=30, ha=\"right\") \n",
    "\n",
    "# Draw rectangles about the significant values. The significant values should be added AUTOMATICALLY\n",
    "\n",
    "from matplotlib.patches import Rectangle, Patch\n",
    "\n",
    "\n",
    "colors = sns.color_palette(\"autumn_r\", 3)\n",
    "## Add rectangles \n",
    "for sq in p005:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[0], lw=2.5))\n",
    "\n",
    "for sq in p001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[1], lw=2.5))\n",
    "\n",
    "for sq in p0001:\n",
    "    ax2.add_patch(Rectangle((sq[0], sq[1]), 1, 1, fill=False, edgecolor=colors[2], lw=2.5))\n",
    "\n",
    "p1 = Patch(edgecolor=colors[0], fill=False, label='p < 0.05', linewidth=2)\n",
    "p2 = Patch(edgecolor=colors[1], fill=False, label='p < 0.01', linewidth=2)\n",
    "p3 = Patch(edgecolor=colors[2], fill=False, label='p < 0.001', linewidth=2)\n",
    "\n",
    "plt.legend([p1, p2, p3], ['p < 0.05','p < 0.01','p < 0.001'], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.1, frameon=False)\n",
    "## Also create a legend for the rectangles. Would be great to have three factors: <0.05, <0.01, <0.001\n",
    "# plt.savefig('groupsdxcluster_againstrest.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('groupsdxcluster_againstrest.eps', format='eps', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
